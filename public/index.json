[
{
	"uri": "https://alexdess.cloud/posts/govc/",
	"title": "GOVC",
	"tags": ["IaaS", "Automation", "VMC"],
	"description": "Speed up your Cloud Deployments - with repeatable automation",
	"content": "Automate your VM Deployments In this Blogpost you will learn to leverage one of my favourite automation tools called \u0026ldquo;GoVC\u0026rdquo; to automate your virtual machine and OVF-Deployments for VMware Cloud on AWS and vSphere. The tool does a great job making in creating repeatable deployments of VMs and OVFs.\nIf you are working with lab environments itÂ´s key to have a repeatable and reliable way to deploy your VMs without having to enter a lot data over and over again. I guess thereÂ´s not a single admin out there who hasnÂ´t klicked thourgh a OVA-Deployment wizard multiple times because some minor setting was wrong causing the deployment to fail ;). From this day on you will want to use GoVC and never face this problem again.\nIn my example below, weÂ´re using GoVC to preapre a ubuntu-Image for vSphere and clone it a few times. My machine I am operating from is a ubuntu running inside my VMware Cloud on AWS SDDC. You can use any OS as long as you have connectivity to your vCenter. First things first - Install GoVC LetÂ´s start by downloading GoVC for our operating system. The easiest way is to go to the github page and pick the appropirate download for your OS.\nGo to Github and determine the Version neccessary for your OS. https://github.com/vmware/govmomi/releases\nDownload the Version for your OS, make it executable and move it into your path:\n curl -L https://github.com/vmware/govmomi/releases/download/prerelease-v0.21.0-58-g8d28646/govc_linux_amd64.gz| gunzip \u0026gt; /usr/local/bin/govc # make govc executable chmod +x /usr/local/bin/govc  Allow communication In order to do deployments from our Jumphost where we just installed with GoVC, we need to setup some firewall rules. GoVC will first talk to the vCenter but the actual deployment happens via a direct communication to the ESXi host so we have to open the firewall for these ports. In my example below I am showing how to achieve this in the VMC-Console of our SDDC.\n In the VMC console navigate to:  Network and Security Gateway Firewall Management Gateway    and setup the following rules to allow the incomming traffic from our Jumphost:\nTo the vCenter Server: To the ESXi-Hosts: Configure your vCenter Connection A huge benefit of GoVC is that you can work with environment variables. This enables you to easily switch between multiple environments. We will create a configuration file which contains the connection details for your vCenter in order to leverage it for the deployment.\nCreate a text file with your favourite editor which contains the following data:\ncat vmc-govcvars.sh #!/bin/bash export GOVC_INSECURE=1 # In order to NOT verify SSL certs on vCenter export GOVC_URL=https://vcenter.sddc-XXXXXXXXX.vmwarevmc.com/sdk # Your SDDC / vCenter IP or FQDN export GOVC_USERNAME=cloudadmin@vmc.local # vCenter username export GOVC_PASSWORD=HIGHLYSECURE123 # vCenter password export GOVC_DATASTORE=WorkloadDatastore # Default datastore to deploy to - Neccessary for deployments to VMC! export GOVC_NETWORK=EMEA-VMC-TEST-04-DHCP # Default network to deploy to export GOVC_RESOURCE_POOL=/SDDC-Datacenter/host/Cluster-1/Resources/Compute-ResourcePool # Default resource pool to deploy t export GOVC_FOLDER=adess # Default folder for our deployments  source vmc-govcvars.sh  As a last thing we want to check if govc was successfully installed:\nvmware@adess-jh:~/deploy$ govc about Name: VMware vCenter Server Vendor: VMware, Inc. Version: 6.8.7 Build: 13619665 OS type: linux-x64 API type: VirtualCenter API version: 6.7.2 Product ID: vpx UUID: 56d58607-fd5c-4206-84cb-acb4ed88276f  If this command shows you information about your vCenter you have successfully installed and configured GoVC. {\u0026hellip;woohooo!!!}\nCreate a template and start automating! Now that we verified the successfull setup of GoVC, we can create our vSphere template. We will use a ubuntu server image which is slim, functional and can be leveraged to create a vSphere template. You can use any appliance or OVA-Template here :)!\nThe next steps will download our OVA Image to the Jumphost and prepare it for deployment:\n# download the ova image: wget https://cloud-images.ubuntu.com/releases/18.04/release/ubuntu-18.04-server-cloudimg-amd64.ova # Extract the ova-specs from the image to a \u0026quot;ubuntu1.json\u0026quot; file govc import.spec ubuntu-18.04-server-cloudimg-amd64.ova | jq . \u0026gt; ubuntu1.json Now we can edit the specification and specify a network or a SSH-Key. Note that for ubuntu you HAVE to specify your public key in order to logon after deployment. vmware@adess-jh:~/deploy$ cat ubuntu.json { \u0026quot;DiskProvisioning\u0026quot;: \u0026quot;thin\u0026quot;, \u0026quot;IPAllocationPolicy\u0026quot;: \u0026quot;dhcpPolicy\u0026quot;, \u0026quot;IPProtocol\u0026quot;: \u0026quot;IPv4\u0026quot;, \u0026quot;PropertyMapping\u0026quot;: [ { \u0026quot;Key\u0026quot;: \u0026quot;instance-id\u0026quot;, \u0026quot;Value\u0026quot;: \u0026quot;id-ovf\u0026quot; }, { \u0026quot;Key\u0026quot;: \u0026quot;hostname\u0026quot;, \u0026quot;Value\u0026quot;: \u0026quot;ubuntu1804\u0026quot; }, { \u0026quot;Key\u0026quot;: \u0026quot;seedfrom\u0026quot;, \u0026quot;Value\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Key\u0026quot;: \u0026quot;public-keys\u0026quot;, \u0026quot;Value\u0026quot;: \u0026quot;ssh-rsa AAAAB3NzaC1ycXXXX/ZXgTKs7LhVNc+Od5SPl98PkviU3lUsgkt5tavyyR5Qbxqs+r6zbZKfXXXXXD/Q4djNPEEnZJ6Qw4SJIQn/Ygpps6CsvGrccYAC7XXXXXXXXXXXblBs5qXBhr+It/73 vmware@adess-jh\u0026quot; }, { \u0026quot;Key\u0026quot;: \u0026quot;user-data\u0026quot;, \u0026quot;Value\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Key\u0026quot;: \u0026quot;password\u0026quot;, \u0026quot;Value\u0026quot;: \u0026quot;VMware1!\u0026quot; } ], \u0026quot;NetworkMapping\u0026quot;: [ { \u0026quot;Name\u0026quot;: \u0026quot;VM Network\u0026quot;, \u0026quot;Network\u0026quot;: \u0026quot;EMEA-VMC-TEST-04-DHCP\u0026quot; } ], \u0026quot;MarkAsTemplate\u0026quot;: false, \u0026quot;PowerOn\u0026quot;: true, \u0026quot;InjectOvfEnv\u0026quot;: false, \u0026quot;WaitForIP\u0026quot;: false, \u0026quot;Name\u0026quot;: \u0026quot;ubuntu1804-template\u0026quot; }  After we did the specification we can now we can deploy the machine to vSphere:\ngovc import.ova -options=ubuntu.json ubuntu-18.04-server-cloudimg-amd64.ova  The cool thing is once you created a config-file for a OVA like we did above you can deploy n-number of machines from it without having to specify all parameters over and over again.\nUsing GoVC for vSphere Cloning As we seen above, govc is great to automate the deployment of OVA-Images but you can leverage it to basically automate any vShere Operation. Below i want to show you a few very helpful operations: Clone multiple VMs from a Template We will use govc to clone 4 VMs from our newly created template with our guest-customization \u0026ldquo;Ubuntu\u0026rdquo;.\ngovc vm.clone -vm ubuntu1804-template -customization=Ubuntu k8s-master01 govc vm.clone -vm ubuntu1804-template -customization=Ubuntu k8s-worker01 govc vm.clone -vm ubuntu1804-template -customization=Ubuntu k8s-worker02 govc vm.clone -vm ubuntu1804-template -customization=Ubuntu k8s-worker03  Power VMs on and of: govc vm.power -on=true k8s-master01  Search a VM by IP govc find . -type m -guest.ipAddress \u0026quot;172.30.118.34\u0026quot; -runtime.powerState poweredOn /SDDC-Datacenter/vm/Workloads/adess/k8s-master01  Get detailed information about a VM: govc vm.info k8s-master01 Name: k8s-master01 Path: /SDDC-Datacenter/vm/Workloads/adess/k8s-master01 UUID: 422597fd-d975-99d0-49af-6a3074d53415 Guest name: Ubuntu Linux (64-bit) Memory: 4096MB CPU: 4 vCPU(s) Power state: poweredOn Boot time: \u0026lt;nil\u0026gt; IP address: 172.30.118.34 Host: 10.56.32.4  Upload a file to a Datastore: The command below will upload our ubuntu-ova image to the Workloaddatastore in the directory /isos\ngovc datastore.upload ubuntu-18.04-server-cloudimg-amd64.ova /isos/ubuntu.ova  Info for ResourcePool govc pool.info Compute-ResourcePool Name: Compute-ResourcePool Path: /SDDC-Datacenter/host/Cluster-1/Resources/Compute-ResourcePool CPU Usage: 12618MHz (5.6%) CPU Shares: normal CPU Reservation: 0MHz (expandable=true) CPU Limit: -1MHz Mem Usage: 368534MB (26.2%) Mem Shares: normal Mem Reservation: 0MB (expandable=true) Mem Limit: -1MB  I hope this post will help you to automate your daily tasks in your SDDC .\nUseful Links. https://github.com/vmware/govmomi\n"
},
{
	"uri": "https://alexdess.cloud/",
	"title": "Whoami?",
	"tags": [],
	"description": "",
	"content": "Welcome to my Blog!  apiVersion: SE/v1 kind: Who-AM-I metadata: name: Alexander Dess title: Senior VMware Cloud Solutions Engineer BU: VMware Cloud Business Unit eMail: adess@vmware.com twitter: @dessCloud Github: appdess replicas: 1  I am an Systems Engineer working for VMware and passionate about Cloud technologies and Cloud Native application architecture. In this blog I want to share my experiences, tools and methodologies I am using during my daily work. All the articles are there to give you guidance in order to leverage and understand the technologies.\nPlease do not hesistate to contact me via Linkedin, Twitter or Email if you have suggestions or any further questions.\n"
},
{
	"uri": "https://alexdess.cloud/tags/k8s/",
	"title": "K8s",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/tags/monitoring/",
	"title": "monitoring",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/posts/",
	"title": "Posts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/posts/from-metrics-to-insights/",
	"title": "Prometheus - From Metrics to Insights",
	"tags": ["K8s", "VMC", "monitoring"],
	"description": "Prometheus Monitoring with TKG+ on VMC",
	"content": "Leveraging the Prometheus Operator on VMC Prometheus was the second project which joined the Cloud Native Computing foundation after Kubernetes in 2016. Originally it was founded 2012 at Soundcloud but is now completely vendor independent and open source. A lot companies have already adopted Prometheus, and the project has a highly active community. Customers which are running VMware Tanzu Kubernetes Grid+ are provided with full Enterprise support included with their TKG+ subscription.\nWhile Prometheus is free it doesn\u0026rsquo;tÂ´t mean that itÂ´s easy and straightforward to understand and to deploy. This is why I made this blogpost to get your Prometheus instance on VMware-Cloud and any K8s environment up and running in - letÂ´s say less than 4 Minutes ;).\nTo start with this tutorial you will need:\n Helm installed on your machine TKG+ or another K8s Cluster using the CSI Storage driver on vSphere or VMware-Cloud A cup of coffee and 4-6 Minutes time ðŸ™‚   Prometheus Architecture In this blogpost we will deploy prometheus and visualise our K8s cluster metrics like the CPU utilisation or the utilisation of our Persistent Volumes with Grafana.\nLetÂ´s have a quick look at the prometheus architecture below. I simplified the diagram to the components which we will focus on later in the tutorial:\n The Prometheus Server which stores our time-series data Grafana to visualise your collected metrics The Altermanager component to get you out of bed if necessary ;)  The picture below shows the Architecture of Prometheus: Deploy the Operator We are going to leverage the Prometheus operator deployed via a helm chart. It packages up all the components of Prometheus which are necessary for our logging stack.\nIf you donÂ´t know what Helm is I recommend you to check out this Link. If you want to learn more about the Operator-Pattern in K8s i recommend to check out this link. Cloud Native Storage for our metrics We want to persist our metrics and will make use of the Cloud-Native Storage Integration of vSphere. Notice that you need at least vCenter version 6.7U3 which provides the required API.\nFirst we want to make sure that we have a Storage Class created accordingly in K8s pointing to a Datastore in vSphere.\nIf you have a usable default Storage Class already you can skip this step. You may want to check if you have a Storage-Class already available by running:\n$ kubectl get sc NAME PROVISIONER default (default) csi.vsphere.vmware.com If you have a working default Storage class for your environment you can skip the creation. Otherwise you can create the storage class accordingly or use the \u0026ldquo;sc.yaml\u0026rdquo; file from my repo and adjust it to fit to your environment.\n# Download the Storage Class example file: $ wget https://raw.githubusercontent.com/appdess/prometheus-operator-vsphere/master/sc.yaml Make sure you replace the value for \u0026ldquo;datastoreurl\u0026rdquo; according to your environment. You can get this value from the vSphere or VMC-UI in the \u0026ldquo;Storage\u0026rdquo; Tab.\n# adjust the \u0026#34;datastoreurl\u0026#34; according to your environment BEFORE you apply the storage-class! kind: StorageClass apiVersion: [storage.k8s.io/v1](http://storage.k8s.io/v1) metadata: name: default # this is the storage class name we will create in K8s namespace: default labels: annotations: [storageclass.kubernetes.io/is-default-class:](http://storageclass.kubernetes.io/is-default-class:) \u0026#34;true\u0026#34; provisioner: [csi.vsphere.vmware.com](http://csi.vsphere.vmware.com/) allowVolumeExpansion: true parameters: storagePolicyName: \u0026#34;vSAN Default Storage Policy\u0026#34; # our vSAN Storage policy from vCenter **datastoreurl**: **\u0026#34;ds:///vmfs/volumes/vsan:e4fdeb26bf93402f-956989e5b4bf358e/**\u0026#34; # the Datasture-URL of our WorkloadDatastore # Apply the Storage Class in your Environment** $ kubectl apply -f sc.yaml # this should output $ storageclass.storage.k8s.io/default created Ok - weÂ´ve successfully created a Storage Class which points to our vSAN Datastore and leverages the \u0026ldquo;vSAN Default Storage Policy\u0026rdquo; - weÂ´re good to deploy Prometheus.\nDeploy the Prometheus Operator We need to adjust the default deployment to fit to our environment. The great thing leveraging helm is that we can specify a central file called \u0026ldquo;values.yaml\u0026rdquo; which will override the given standard values. This is how you customise deployments with helm to fit to your infrastructure and requirements.\nYou can customise the deployment leveraging a yaml file. My file below is called \u0026ldquo;values.yaml\u0026rdquo; and specifies the standard PW for Grafana, the vSphere Storage Class we created before and a setting for the serviceMonitor discovery of Prometheus.\nIf you are using the Storage Class \u0026ldquo;default\u0026rdquo; in your environment you can simply use my updated file and start the deployment via Helm.\nBelow is my customisation i applied to the original file - you can customise further or skip this step and continue with the helm deployment by leveraging my template file below.\n# OPTIONAL BOX - you may want to use my prepared file below instead of customisig your own # get the latest values.yaml file and edit it accordingly or take my example: wget https://raw.githubusercontent.com/helm/charts/master/stable/prometheus-operator/values.yaml # change the admin PW for Grafana adminPassword: VMware1! # Change the Storage Spec to use our \u0026#34;default\u0026#34; Storage Class  storageSpec: volumeClaimTemplate: spec: storageClassName: default accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] resources: requests: storage: 50Gi # set the value for service-discovery to discover sevice-montors (crds) created in the namespace serviceMonitorSelectorNilUsesHelmValues: false Deploy the Prometheus operator to our K8s Cluster\n# get the values exmaple file which contains the changes from above: $ wget https://raw.githubusercontent.com/appdess/prometheus-operator-vsphere/master/values.yaml # install the Prometheus operator via helm customized by our file: $ helm install prometheus stable/prometheus-operator -f values.yaml # this should output the following: The Prometheus Operator has been installed. Check its status by running: Verify the status of the components:\n$ kubectl --namespace default get pods -l \u0026#34;release=prometheus\u0026#34; NAME READY STATUS RESTARTS AGE prometheus-prometheus-node-exporter-dhcfm 1/1 Running 0 93s prometheus-prometheus-node-exporter-gcn6q 1/1 Running 0 93s prometheus-prometheus-node-exporter-p2gs4 1/1 Running 0 93s prometheus-prometheus-node-exporter-x7ff9 1/1 Running 0 93s prometheus-prometheus-oper-operator-769d757547-vh9kc 2/2 Running 0 93s Configure Access to Grafana LetÂ´s also check the services which have been created by our deployment. We are especially interested in the Grafana service since we want to access the Dashboard to view our collected metrics.\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus-grafana ClusterIP 100.64.44.227 \u0026lt;none\u0026gt; 80/TCP 14m prometheus-kube-state-metrics ClusterIP 100.69.121.82 \u0026lt;none\u0026gt; 8080/TCP 14m prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 14m You can either change the internal Cluster-IP of \u0026ldquo;prometheus-grafana\u0026rdquo; to an external loadbalancer IP or simply make the user interface available leveraging a kubectl port-forwarding to your local machine.\nHow to setup port forwarding to your local machine:\n$ kubectl port-forward deployment/prometheus-grafana 8080:3000 Forwarding from 127.0.0.1:8080 -\u0026gt; 3000 Forwarding from [::1]:8080 -\u0026gt; 3000 You can now access Grafana by opening a browser to http://127.0.0.1:8080\nHow to configure a load balancer for Grafana:\n$ kubectl patch svc prometheus-grafana -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; # check your IP to access Grafana: $ kubectl get svc prometheus-grafana NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S)  prometheus-grafana LoadBalancer 100.66.133.185 **172.30.124.23** 80:31365/TCP You can now access Grafana by opening a browser and point it to http://172.30.124.23 (Your external IP from above)\nDepending which method you choose you access the dashboard either via:\n Port-Forwarding: http://127.0.0.1:8080 LoadBalancer: http://**172.30.124.23** (The IP for your service from above) User: admin | PW: VMware1!  Next we want to add our first dashboard to view our Cluster Resources. You can do this by navigating to \u0026ldquo;Dashboard\u0026rdquo; and \u0026ldquo;Manage\u0026rdquo;.\nNow you can select an example Dashboard. IÂ´ve chosen to select the \u0026ldquo;Cluster Dashboard\u0026rdquo; to get an holistic view about all my K8s Cluster Resources.\nYou will notice there are a lot default templates already available which you can customise due to your needs. You can also drill down on the resources to investigate for example what namespaces and pods are consuming the most memory.\nWhile the above graph shows the memory consumption per namespace, the graph below shows the memory consumption per POD in a given namespace.\nVerify Persistent Storage for Prometheus Containers are stateless per default but our data should not be lost when the containers for our deployment are updated or restarted. LetÂ´s do a quick check if our Persistent Volume was successfully created and the data is written to it accordingly.\nWe are able to do this and view the Persistent Volumes as well as their current storage consumption in the Grafana UI. The dashboard is in the templates folder and simply called \u0026ldquo;Persistent Volumes\u0026rdquo;:\nViewing our Persistent Volume Claim including the usage created with our Storage Class:\nMy \u0026ldquo;prometheus-ingress.yaml\u0026rdquo; below:\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: prometheus namespace: default spec: rules: - host: alertmanager.set.local http: paths: - backend: serviceName: prometheus-prometheus-oper-alertmanager servicePort: 9093 - host: grafana.set.local http: paths: - backend: serviceName: prometheus-grafana servicePort: 80 - host: prometheus.set.local http: paths: - backend: serviceName: prometheus-prometheus-oper-prometheus servicePort: 9090  ThatÂ´s it for the first post - we successfully setup Prometheus with Persistent Storage and Grafana to visualise our data. Thank you for reading and stay tuned for the next topics like alerting, setting up ingress and how to get your application metrics into Prometheus.\n"
},
{
	"uri": "https://alexdess.cloud/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/tags/vmc/",
	"title": "VMC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/tags/storage/",
	"title": "Storage",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/tags/tkg/",
	"title": "TKG",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/posts/tkgonvmc/",
	"title": "TKG on VMC",
	"tags": ["K8s", "VMC", "Storage", "TKG"],
	"description": "TKG 1.0 is GA!",
	"content": "Tanzu Kubernetes Grid is GA! In this blogpost I am going to show you how easy it is to setup, configure and lifecycle enterprise K8s Clusters and applications on VMware Cloud.\nWe will deploy Tanzu Kubernetes Grid to VMConAWS as well as an application which uses Cloud Native vSAN Storage and existing services on VMConAWS to expose it to the outside world.\nBefore we start i would like to introduce you to the basic concepts of Tanzu Kubernetes Grid. Tanzu Kubernetes Grid leverages Cluster API to bring declarative Kubernetes style APIs which can be leveraged for the creation, configuration and management of Kubernetes Clusters itself.\nYou can think of a K8s Cluster - our TKG Management Cluster which is provisioning TKG Workload Clusters. I created a video which you can follow to see the full deployment of a Tanzu Kubernetes Grid on VMware Cloud on AWS including the scaling of a TKG Cluster and the deployment of a application with persistent storage:   For the Jumphost:\n Docker installed - Documentation kubectl installed - Documentation Setup govc (optional but helpful) - see here  For VMConAWS:\n A DHCP enabled network for the TKG-VMs. Example in the Video is \u0026ldquo;TKGonVMC\u0026rdquo;.  Firewall Rules in VMC:\n Compute Gateway:  Setup a FW rule to allow traffic from the network where your jumphost belongs to the \u0026ldquo;TKGonVMC\u0026rdquo; network where your nodes will be deployed - services ANY Setup a FW-Rule to allow traffic from the \u0026ldquo;TKGonVMC\u0026rdquo; network where your nodes will be deployed to the SDDC-Management network - service: ANY Setup a FW-Rule which allows your nodes from the \u0026ldquo;TKGonVMC\u0026rdquo; network to reach out to the Internet- service: ANY   Management Gateway:  Setup a FW-Rule to allow traffic from the \u0026ldquo;TKGonVMC\u0026rdquo; and your Jumphost network to the vCenter via HTTPS   Create a folder and resource pool for the TKG-VMs (optional)   The installation Download TKG: TKG can be downloaded at the official VMware-Pages:\nhttps://vmware.com/go/get-tkg\nYou need to download the necessary components depending on your platform. I am using a ubuntu linux desktop hosted on my VMConAWS environment. Make sure to have network connectivity between your workstation or jumphost from where you are installing your first Management Cluster to vSphere and the TKG-Network we created earlier. I am using a ubuntu horizon desktop running in VMware Cloud on AWS to deploy TKG. Huge thanks to Robert Guske for his great blog-post how to create a linux horizon destkop and my colleague Robert Riemer for introducing me to the mysterious world of Horizon and VDI ;) ! What you need to download: The CLI for your OS (depending on your OS):\n tkg-linux-amd64-v1.0.0_vmware.1.gz  The node-Image for the worker and master nodes:\n photon-3-v1.17.3_vmware.2.ova  The API Server Load Balancer Image:\n photon-3-capv-haproxy-v0.6.3_vmware.1.ova  First we will extract the TKG CLI, make it executable and move it in our path:\n# Extract the CLI: gunzip tkg-linux-amd64-v1.0.0_vmware.1.gz # Rename it and move it in your Path: sudo mv tkg-linux-amd64-v1.0.0_vmware.1 tkg \u0026amp;\u0026amp; sudo cp tkg /usr/local/bin/ # verify the functionality of the CLI tkg version Client: Version: v1.0.0 Git commit: 60f6fd5f40101d6b78e95a33334498ecca86176e  Upload the required VM templates to your vCenter The images for the K8s Nodes and the HA-Proxy need to be present in our vSphere Environment prior the deployment. You can import them via the vSphere UI or simply use govc to achieve this. If you want to get started with govc please have a look at this previous blogpost.\n# extract the available properties from the ova-image: govc import.spec photon-3-capv-haproxy-v0.6.3_vmware.1.ova | jq .\u0026gt; haproxy.json # adjust the two values in the json file to match your Network and Mark it as tempalte! { . \u0026quot;Network\u0026quot;: \u0026quot;TKGonVMC\u0026quot; } \u0026quot;MarkAsTemplate\u0026quot;: true # Import the OVA to vCenter govc import.ova -options=haproxy.json photon-3-capv-haproxy-v0.6.3_vmware.1.ova [14-04-20 12:18:51] Uploading capv-haproxy.ova.vmdk... OK [14-04-20 12:18:51] Marking VM as template... # Deploy the Worker node image the same way govc import.spec photon-3-v1.17.3_vmware.2.ova | jq .\u0026gt; worker.json # Adjust the values Network and MarkAsTemplate like above # Deploy the image to vCenter: govc import.ova -options=worker.json photon-3-v1.17.3_vmware.2.ova govc import.ova -options=worker.json photon-3-v1.17.3_vmware.2.ova [14-04-20 12:23:19] Uploading photon-3-kube-v1.17.3-disk1.ova.vmdk... OK [14-04-20 12:23:19] Marking VM as template...  Launch the TKG installer UI We will leverage the Tanzu-Installer-UI to enter the necessary data for the deployment of the TKG Management Cluster. This is our first Cluster and will provide the possibilities to deploy our actual workload clusters.\nTo start the installer UI simply leverage the following command:\ntkg init --ui  This will start a local webserver and you can begin to enter your values to the wizard.\nSection 1 - IaaS Provider:\n your vCenter Endpoint Username: a User with cloudadmin privileges Click \u0026ldquo;Connect\u0026rdquo; Enter your SSH-Key: for example run \u0026ldquo;cat ~/.ssh/id_rsa.pub\u0026rdquo; in a terminal to determine your users public key or create a new one Select the available SDDC as Datacenter   This are the settings for your Management Cluster which will be the control plane to deploy and manage Kubernetes Clusters. Depending on your selection the components like etcd will be deployed accordingly. Section 3 - Resources:\n Resource Pool: Select the \u0026ldquo;tkg\u0026rdquo; resource Pool VM Folder: Select the folder you created earlier Datastore: Select the workload Datastore  Select your \u0026ldquo;TKGonVMC\u0026rdquo; Network. It needs to have DHCP enabled! You can leave the Cluster-Service and POD-CIDR default or change it according to your preferences.\nHit next and verify if the deployment is successful. This will take approximately 7-10 minutes.\nCheck the status of your management cluster. The installer will automatically set the context for kubectl to the management cluster. You can check whether your management cluster is healthy by viewing the pods running on it:\nkubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-68d68b745-45cqd 2/2 Running 0 4h25m capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-5df79cdfdd-xfcm2 2/2 Running 0 4h25m capi-system capi-controller-manager-69cd5fcdc8-7f5vr 2/2 Running 0 4h25m capi-webhook-system capi-controller-manager-675d6bbb7c-vxgzz 2/2 Running 0 4h25m capi-webhook-system capi-kubeadm-bootstrap-controller-manager-658dcb9d78-js9p6 2/2 Running 0 4h25m capi-webhook-system capi-kubeadm-control-plane-controller-manager-5f94cdcf78-tt86p 2/2 Running 0 4h25m capi-webhook-system capv-controller-manager-db5c995bc-m9bh2 2/2 Running 0 4h25m capv-system capv-controller-manager-76d9454f9d-vx66f 2/2 Running 0 4h25m cert-manager cert-manager-567cc74dfb-84m6g 1/1 Running 0 4h27m cert-manager cert-manager-cainjector-59f464c7ff-764c9 1/1 Running 0 4h27m cert-manager cert-manager-webhook-549dc474bc-q2lhs 1/1 Running 0 4h27m kube-system calico-kube-controllers-7986b8994b-hsxg4 1/1 Running 0 4h27m kube-system calico-node-995bz 1/1 Running 0 4h26m kube-system calico-node-crhtv 1/1 Running 0 4h27m kube-system coredns-9d6cb6b59-ndlb7 1/1 Running 0 4h27m kube-system coredns-9d6cb6b59-s7p99 1/1 Running 0 4h27m kube-system etcd-tkg-management-control-plane-6nwpx 1/1 Running 0 4h27m kube-system kube-apiserver-tkg-management-control-plane-6nwpx 1/1 Running 0 4h27m kube-system kube-controller-manager-tkg-management-control-plane-6nwpx 1/1 Running 0 4h27m kube-system kube-proxy-b4ms9 1/1 Running 0 4h27m kube-system kube-proxy-v29vj 1/1 Running 0 4h26m kube-system kube-scheduler-tkg-management-control-plane-6nwpx 1/1 Running 0 4h27m kube-system vsphere-cloud-controller-manager-gfd9t 1/1 Running 0 4h27m kube-system vsphere-csi-controller-0 5/5 Running 0 4h27m kube-system vsphere-csi-node-lttgr 3/3 Running 0 4h26m kube-system vsphere-csi-node-nv74d 3/3 Running 0 4h26m  Looks good? - GREAT! This means itÂ´s time to deploy our first workload Cluster :).\nThe command below will create a development Cluster with 1 Master and 1 Worker node named \u0026ldquo;tkg-workload-01\u0026rdquo;.\ntkg create cluster --plan=dev tkg-workload-01  Once the deployment is done the context will be switched to the workload cluster and you should be able to view all the pods running as well.\nYou can use the following command to switch between existing kubeconfig contexts from now on to manage your clusters:\nkubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO * tkg-management-admin@tkg-management tkg-management tkg-management-admin tkg-workload-01-admin@tkg-workload-01 tkg-workload-01 tkg-workload-01-admin # to switch between cluster-contexts: kubectl config use-context tkg-workload-01-admin@tkg-workload-01 Switched to context \u0026quot;tkg-workload-01-admin@tkg-workload-01\u0026quot;  Persistent Storage for Kubernetes A great feature which is provided out of the Box with TKGonVMC is, that we are able to leverage the Cloud-Native storage integration of VMConAWS to provide vSAN Storage to our K8s applications. All we need to do is to create a storage-Class in K8s which we can use and which points to the WorkloadDatastore with the correct policy.\nThe Storage-Class below will create a storage class with the default vSAN-Policy pointing to the WorkloadDatastore. Make sure you replace the values according to your environment.\ncat sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: standard annotations: storageclass.kubernetes.io/is-default-class: \u0026quot;true\u0026quot; provisioner: csi.vsphere.vmware.com allowVolumeExpansion: true parameters: storagePolicyName: \u0026quot;vSAN Default Storage Policy\u0026quot; datastoreurl: \u0026quot;ds:///vmfs/volumes/vsan:b01e02424695410d-96775a9212a96fa1/\u0026quot;  To apply this storage class to your K8s Cluster all you have to do is to run the following command:\nkubectl apply -f sc.yaml storageclass.storage.k8s.io/standard created  Great - from now on weÂ´re able to leverage the storage class \u0026ldquo;standard\u0026rdquo; in our K8s deployments. To check whether this is working you can create a simple persistent volume leveraging the following yaml-file:\n# apply the following configuration file \u0026quot;pvc.yaml\u0026quot; apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myfirstpvc namespace: default labels: app: myfirstapp spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 3Gi $ kubectl apply -f pvc.yaml $ persistentvolumeclaim/myfirstpvc created # check if the pvc is successfully bound $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE myfirstpvc Bound pvc-949b6d58-4a6e-4496-bb0e-a203a741acb7 3Gi RWO standard 15s  When you apply this file and it showÂ´s bound you will also recognise that the K8s volume is now visible in the vSphere UI when you navigate to \u0026ldquo;WorkloadDatastore â†’ Monitor â†’ Container Volumes\u0026rdquo;. Notice that the K8s label we set for our pvc is directly visible in vSphere. This makes it very easy to identify the applications which are using this pvc later on.\nWe successfully deployed Tanzu Kubernetes grid on VMware Cloud on AWS and can now create, delete or update Kubernetes Clusters leveraging the tight integration between vSphere and Kubernetes.\nThe video I made is also handling the scaling of a TKG-Cluster and the deployment of a example chat application called rocketchat.\nUseful Links: Download:\n vmware.com/go/get-tkg  Product Documentation:\n https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/index.html  Release Notes:\n https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.0/rn/VMware-Tanzu-Kubernetes-Grid-10-Release-Notes.html  Cluster-API:\n https://github.com/kubernetes-sigs/cluster-api  Youtube-Video:\n https://youtu.be/roHaVR-yOw8  How to create a Horizon Linux Development Desktop:\n https://rguske.github.io/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/  "
},
{
	"uri": "https://alexdess.cloud/posts/app-modernization-with-vmc/",
	"title": "App modernization with VMC",
	"tags": ["K8s", "VMC", "cluster-api", "capv"],
	"description": "A application modernisation example leveraging VMware Cloud and K8s",
	"content": "In this blogpost weÂ´re going to perform an exemplary application modernization. I wanted to provide an easy and understandable example how an existing application can be modernized in order to take advantages of modern application architecture by leveraging Kubernetes.\nModernising parts of an existing application is what I observed in real world customer scenarios. Parts of an application are modernized to take the maximum value compared to the high effort of refactoring the whole application. There are a variety of reasons why a modernization makes sense. Common examples would be to enable the use of features like auto-scaling and monitoring or the decomposition of single modules because they became too big and complex for a single team to fully understand and maintain.\nI recommend to read the blogpost first and then watch the video.\n  Base Case Our company in this showcase is called \u0026ldquo;Commute Analytics\u0026rdquo; and is providing a service to identify which is the best time in order to start your journey when commuting. The company chose to move all their workloads to the cloud since their contracts for the Datacenter are expiring and they chose not to invest in a new datacenter location. Workload Migration The first thing our Company needs to achieve is the evacuation of their current Datacenter. After a successful pilot Commute Analytics chose to leverage VMware Cloud on AWS with VMware HCX to migrate all their workloads to the cloud. Our example starts after the workloads have been migrated from on premises to VMware Cloud on AWS with HCX. If you donÂ´t know what HCX is i highly recommend to read this article. How does the current application look-like? Our application is accessible for users at \u0026ldquo;http://commuteanalytics.corp.local\u0026rdquo;. Every user can enter the source and destination address of their journey and will get recommendations when itÂ´s best to start driving.\nTackling the application problems After the successful migration to VMware Cloud on AWS the company is still facing problems with their application and users are complaining about performance during peak times. The current application architecture looks like the following: A analysis of the application has shown that:\n The current application architecture limits throughput and does not scale to the amount of users Upgrades are difficult to achieve and always require a downtime The bottleneck of the application is the frontend which is not capable of handling the amount of users during peak times  Since our bottleneck is the frontend of the application weÂ´re going to modernize this part of the application while leaving the Databse as a VM.\nPrerequisites: Running Kubernetes on VMware Cloud on AWS We choose to modernize the frontend and migrate this part of the application to K8s. Therefore we are leveraging Tanzu Kubernetes Grid (former Essentials PKS) - VMwareÂ´s upstream Kubernetes distribution which helps to provide a common Kubernetes distribution across all your environments. I will write a seperate blogpost about how to deploy TKG on VMC leveraging Cluster-API and the options youÂ´re having for Load Balancing and Ingress controllers in the near future.\nThe picture below shows TKG on VMConAWS with one workload-cluster roled out as well as the current application VMs \u0026ldquo;tito-db\u0026rdquo; and \u0026ldquo;tito-fe\u0026rdquo;. Prepare the Frontend In order to enable a smooth migration weÂ´re leveraging the existing Apache configuration of the frontend VM \u0026ldquo;tito-fe\u0026rdquo; and creating a container which will act as our new frontend.\nThe next thing is to build our container image for the new frontend.LetÂ´s have a look at our Dockerfile - you see that weÂ´re copying the apache config in line 5 to the container image. As a next step we build our container image and upload it to the registry of choice. For this Demo I chose to simply use Docker hub. If you wanna try this at home you will find prebuild containers on my docker Docker registry  Additional Components In order to make our application scalable we need some additional components. Most important weÂ´re switching from the single VM to a load balanced approach leveraging K8s. To create the Load Balancer in K8s we will apply the following file to our K8s Cluster. The actual Load Balancer is then automatically implemented by the AVI networks solution which is deployed on the cluster. In terms of reliability weÂ´re also setting a fixed IP for our Load Balancer in Line 27.  kubectl apply -f tito-lb.yaml  Application deployment Right now our old 2-Tier application is still running and serving users. At the same time weÂ´re going to deploy our new K8s based frontend which will use the existing database in the backend. This procedure is completly transparent for the users which are using the service since they are still served by the old frontend VM.\nLetÂ´s deploy our K8s based frontend and point it to the IP of our existing MySQL VM:  kubectl apply -f tito-deployment.yaml  Migrate the Users Right now our application is accessible by the old DNS entry for our users but also via the new Load Balancer IP. This is possible because the frontend is completly stateless. A huge benefit is that we access the application via the IP and are able to verify and test the funcionality of the application.\nAccess via K8s LB-IP: After we confirmed the correct functionality of our application we can simply migrate our users by changing our DNS record from the VM to our new K8s LB IP and our clients will be moved uninterrupted to the new K8s based frontend. Leverage the new Application architecture Our modernized application does now look like the following: LetÂ´s remember why we wanted to modernize the application in the first place. There have been scalability problems and users complained about bad performance during peak times. To tackle this we can now use K8s build in functionality like the horizontal pod autoscaler or simply scale our deployment up manually because weÂ´re expecting higher load during peak times.\nAs we can see below the deployment was scaled up from the inital two to 10 replicas (containers) and we can immediately see the CPU load dropping in Grafana. I hope you enjoyed reading this blogpost. Stay tuned - in the next blogposts i would like to show you how you can improve visibility for your K8s applications via Prometheus and Grafana and how to attach your existing clusters to Tanzu Mission Control.\n#VMwarePKSonVMC #TKG #VMware\n"
},
{
	"uri": "https://alexdess.cloud/tags/capv/",
	"title": "capv",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/tags/cluster-api/",
	"title": "cluster-api",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/posts/cluster-api/",
	"title": "K8s ClusterAPI",
	"tags": ["K8s", "VMC", "Storage"],
	"description": "State of the art povisioning for K8s Clusters",
	"content": "What is the CAPI Project about? ClusterAPI is a project focussing on the lifecycle management day0 to day2 in order to ease the management for K8s Clusters. The project explains itself with:\n The Cluster API is a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management.\n Get the environment ready In order to leverage all the functionality please make sure youÂ´re using the latest version.\n vSphere \u0026gt; 6.7 U3 (Cloud Native-Storage was introduced in this Version) govc installed and configured - please see This Post kind (kubernetes in docker)  CSI Components: The drawing below is a logical drawing regarding the neccessary components and high-level interaction between those.\nIn this tutorial we will leverage the csi-vsphere which enables the provisioning of those static values on demand. For the current K8s version the CSI-Plugin is the preferred way to be leveraged.\nCheck your versions and tooling  vSphere \u0026gt; 6.5 K8s \u0026gt; 1.14 (IÂ´m using 1.15.2) govc installed and configured - please see This Post  First step is to get the vsphere-csi-driver WeÂ´re going to clone the vSphere CSI-Driver plugin to your nodes. You can clone this to all of them or distribute via scp\ngit clone https://github.com/kubernetes-sigs/vsphere-csi-driver.git  Enable Disk UUID for all your Nodes If you followed my previous post you have most likely already enabled this setting. If you havenÂ´t please enable the disk.enableUUID=1\ngovc vm.change -vm k8s-master01 -e \u0026quot;disk.enableUUID=1\u0026quot; govc vm.change -vm k8s-worker01 -e \u0026quot;disk.enableUUID=1\u0026quot; govc vm.change -vm k8s-worker02 -e \u0026quot;disk.enableUUID=1\u0026quot;  Providing the vSphere Credentials to the CSI-Plugin There are two options how to provide the csi-vsphere our vCenter Credentials.\n Using a K8s Secret Within the vSphere.conf  In our case we will provide the secrets in the vsphere.conf since weÂ´re working in our lab-environment. For a real environment we would for sure store the credentials in a K8s secret.\nMy vsphere.conf:\n[Global] user = \u0026quot;cloudadmin@vmc.local\u0026quot; password = \u0026quot;Highlysecure-XXXX-\u0026quot; port = \u0026quot;443\u0026quot; insecure-flag = \u0026quot;1\u0026quot; [VirtualCenter \u0026quot;10.56.224.4\u0026quot;] datacenters = \u0026quot;SDDC-Datacenter\u0026quot; [Workspace] server = \u0026quot;10.56.224.4\u0026quot; datacenter = \u0026quot;SDDC-Datacenter\u0026quot; default-datastore = \u0026quot;WorkloadDatastore\u0026quot; resourcepool-path = \u0026quot;SDDC-Datacenter/host/Cluster-1/Resources/Compute-ResourcePool\u0026quot; folder = \u0026quot;adess\u0026quot;  Create configmap:\nCreate the RBAC roles Next we need to create the neccessary rbac roles\nubuntu@k8s-master01:~/vsphere-csi-driver$ kubectl create -f manifests/1.14/rbac serviceaccount/vsphere-csi-attacher created clusterrole.rbac.authorization.k8s.io/external-attacher-runner created clusterrolebinding.rbac.authorization.k8s.io/csi-attacher-role created serviceaccount/vsphere-csi-provisioner created clusterrole.rbac.authorization.k8s.io/external-provisioner-runner created clusterrolebinding.rbac.authorization.k8s.io/csi-provisioner-role created serviceaccount/vsphere-csi-controller created clusterrole.rbac.authorization.k8s.io/vsphere-csi-controller-role created clusterrolebinding.rbac.authorization.k8s.io/vsphere-csi-controller-binding created  Wohoo! WeÂ´re now ready to deploy our csi-vsphere Deploy from the git-repo. ItÂ´s mandatory to have the cluster setup with kubeadm in order to properly work:\nkubectl create -f manifests/1.14/deploy statefulset.apps/vsphere-csi-attacher created statefulset.apps/vsphere-csi-controller created [csidriver.storage.k8s.io/vsphere.csi.vmware.com](http://csidriver.storage.k8s.io/vsphere.csi.vmware.com) created daemonset.apps/vsphere-csi-node created statefulset.apps/vsphere-csi-provisioner created  Next we need to create a Storage Class Adjust the values according to your deployment. For the deployment on VMC it is absolutely mandatory that weÂ´re selecting the Workload-Datastore\nubuntu@k8s-master01:~/csi$ cat sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: vsan namespace: kube-system annotations: storageclass.kubernetes.io/is-default-class: \u0026quot;true\u0026quot; provisioner: vsphere.csi.vmware.com parameters: parent_type: \u0026quot;Datastore\u0026quot; parent_name: \u0026quot;DatastoreWorkloadDatastore\u0026quot; # Apply to create our Storage-Class: kubectl apply -f sc.yaml  After creating a storage-class into the K8s-API we can now create a persistent Volume Claim\ncat pvc.yaml: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: adess-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: vsan  Lets check if our volume was created accordingly. If you get the status \u0026ldquo;Bound\u0026rdquo; it has been successfully created. At the same time you should see a vmdk created in vCenter. ubuntu@k8s-master01:~/csi$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE adess-pvc2g Bound pvc-75199f1d-1e9a-4290-bf77-eb7475ba327a 2Gi RWO vsan 5s  ThatÂ´s it - stay tuned. In the next post we will leverage this to deploy a stateful Kubernetes application!\n"
},
{
	"uri": "https://alexdess.cloud/tags/automation/",
	"title": "Automation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/tags/iaas/",
	"title": "IaaS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://alexdess.cloud/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
}]